/**
 * External tokenizer codegen for Lezer
 *
 * Generates JavaScript external tokenizer implementations for Lezer grammars
 * that use `externals` in their language definition.
 *
 * Currently supports:
 * - Indentation-based tokens (indent, dedent, newline)
 */

import type { LanguageDefinition, RuleDefinition, RuleBuilder } from '../../definition/index.js';
import { createBuilderProxy } from '../../definition/grammar.js';
import { toPascalCase } from './grammar.js';

type RuleNode =
  | { type: 'string'; value: string }
  | { type: 'regex'; value: RegExp }
  | { type: 'seq'; rules: RuleNode[] }
  | { type: 'choice'; rules: RuleNode[] }
  | { type: 'optional'; rule: RuleNode }
  | { type: 'repeat'; rule: RuleNode }
  | { type: 'repeat1'; rule: RuleNode }
  | { type: 'field'; name: string; rule: RuleNode }
  | { type: 'prec'; level: number; rule: RuleNode }
  | { type: 'prec.left'; level: number; rule: RuleNode }
  | { type: 'prec.right'; level: number; rule: RuleNode }
  | { type: 'prec.dynamic'; level: number; rule: RuleNode }
  | { type: 'token'; pattern: string | RegExp }
  | { type: 'token.immediate'; rule: RuleNode }
  | { type: 'alias'; rule: RuleNode; name: string }
  | { type: 'rule'; name: string };

class ExternalTokenBuilder<T extends string> {
  _proxy: RuleBuilder<T> | null = null;
  seq(...rules: RuleDefinition<T>[]): RuleNode { return { type: 'seq', rules: rules.map(r => this.normalize(r)) }; }
  choice(...rules: RuleDefinition<T>[]): RuleNode { return { type: 'choice', rules: rules.map(r => this.normalize(r)) }; }
  optional(rule: RuleDefinition<T>): RuleNode { return { type: 'optional', rule: this.normalize(rule) }; }
  repeat(rule: RuleDefinition<T>): RuleNode { return { type: 'repeat', rule: this.normalize(rule) }; }
  repeat1(rule: RuleDefinition<T>): RuleNode { return { type: 'repeat1', rule: this.normalize(rule) }; }
  field(name: string, rule: RuleDefinition<T>): RuleNode { return { type: 'field', name, rule: this.normalize(rule) }; }
  prec = Object.assign(
    (level: number, rule: RuleDefinition<T>): RuleNode => ({ type: 'prec', level, rule: this.normalize(rule) }),
    {
      left: (level: number, rule: RuleDefinition<T>): RuleNode => ({ type: 'prec.left', level, rule: this.normalize(rule) }),
      right: (level: number, rule: RuleDefinition<T>): RuleNode => ({ type: 'prec.right', level, rule: this.normalize(rule) }),
      dynamic: (level: number, rule: RuleDefinition<T>): RuleNode => ({ type: 'prec.dynamic', level, rule: this.normalize(rule) }),
    }
  );
  token = Object.assign(
    (pattern: string | RegExp): RuleNode => ({ type: 'token', pattern }),
    { immediate: (rule: RuleDefinition<T>): RuleNode => ({ type: 'token.immediate', rule: this.normalize(rule) }) }
  );
  alias(rule: RuleDefinition<T>, name: string): RuleNode { return { type: 'alias', rule: this.normalize(rule), name }; }
  rule(name: T): RuleNode { return { type: 'rule', name }; }
  private normalize(rule: RuleDefinition<T>): RuleNode {
    if (typeof rule === 'string') return { type: 'string', value: rule };
    if (rule instanceof RegExp) return { type: 'regex', value: rule };
    if (typeof rule === 'function') {
      const r = this._proxy ?? (this as unknown as RuleBuilder<T>);
      return rule(r) as unknown as RuleNode;
    }
    return rule as unknown as RuleNode;
  }
}

/**
 * Check if a set of external token names represents an indentation pattern
 */
function isIndentationPattern(names: Set<string>): boolean {
  return names.has('indent') && names.has('dedent') && names.has('newline');
}

/**
 * Generate external tokenizer JavaScript source
 *
 * Returns null if the definition has no externals.
 * Returns the JS source string for the external tokenizer module.
 */
export function generateExternalTokens<T extends string>(
  definition: LanguageDefinition<T>
): string | null {
  if (!definition.externals) return null;

  const builder = createBuilderProxy<T>(new ExternalTokenBuilder<T>());
  const rawExternals = definition.externals(builder);

  const externalNames = new Set<string>();
  for (const item of rawExternals) {
    const node = typeof item === 'string'
      ? { type: 'string' as const, value: item }
      : (item as RuleNode);
    if (node.type === 'rule') {
      externalNames.add(node.name);
    }
  }

  if (externalNames.size === 0) return null;

  // Generate imports for term IDs
  const termImports = [...externalNames]
    .map(name => toPascalCase(name))
    .join(', ');

  if (isIndentationPattern(externalNames)) {
    return generateIndentationTokenizer(termImports, externalNames);
  }

  // Generic stub for unknown external tokens
  return generateGenericStub(termImports, externalNames);
}

/**
 * Generate an indentation-based external tokenizer
 * Handles indent, dedent, and newline tokens
 */
function generateIndentationTokenizer(
  termImports: string,
  _names: Set<string>,
): string {
  return `// External tokenizer for indentation-based parsing
// Generated by treelsp — do not edit
import { ExternalTokenizer, ContextTracker } from "@lezer/lr";
import { ${termImports} } from "./parser.terms.js";

const newlineChar = 10;  // '\\n'
const space = 32;        // ' '
const tab = 9;           // '\\t'

// Peek at the indentation (number of leading whitespace columns) at the
// current input position. Does NOT advance the input.
function peekIndent(input) {
  let indent = 0;
  let offset = 0;
  for (;;) {
    const ch = input.peek(offset);
    if (ch === space) { indent++; offset++; }
    else if (ch === tab) { indent += 8; offset++; }
    else break;
  }
  return indent;
}

// Context tracker maintains the indentation stack.
// Context objects are IMMUTABLE — shift/reduce always return new objects.
export const trackIndent = new ContextTracker({
  start: { stack: [0], pendingIndent: false, pendingDedents: 0 },

  shift(context, term, _stack, input) {
    if (term === Newline) {
      // The input in shift() is positioned at the START of the token, not the end.
      // Skip past the Newline content (\\n + blank lines) to reach indentation whitespace.
      let offset = 0;
      // Skip past newlines and blank-line whitespace
      while (input.peek(offset) === newlineChar) offset++;
      // Check for blank lines with whitespace
      while (offset < 10000) {
        const ch = input.peek(offset);
        if (ch === space || ch === tab) {
          // Could be blank line whitespace — peek ahead
          let wsEnd = offset;
          while (input.peek(wsEnd) === space || input.peek(wsEnd) === tab) wsEnd++;
          if (input.peek(wsEnd) === newlineChar) {
            offset = wsEnd + 1; // skip past the blank line
            continue;
          }
          break; // Content line — stop
        }
        break;
      }
      // Now count indentation at the content line
      let indent = 0;
      while (true) {
        const ch = input.peek(offset);
        if (ch === space) { indent++; offset++; }
        else if (ch === tab) { indent += 8; offset++; }
        else break;
      }
      const currentIndent = context.stack[context.stack.length - 1] ?? 0;

      if (indent > currentIndent) {
        return {
          stack: [...context.stack, indent],
          pendingIndent: true,
          pendingDedents: 0,
        };
      }
      if (indent < currentIndent) {
        const newStack = context.stack.slice();
        let dedents = 0;
        while (newStack.length > 1 && (newStack[newStack.length - 1] ?? 0) > indent) {
          newStack.pop();
          dedents++;
        }
        return { stack: newStack, pendingIndent: false, pendingDedents: dedents };
      }
      // Same level — clear any stale pending flags
      if (context.pendingIndent || context.pendingDedents > 0) {
        return { stack: context.stack, pendingIndent: false, pendingDedents: 0 };
      }
      return context;
    }
    if (term === Indent) {
      if (context.pendingIndent) {
        return { stack: context.stack, pendingIndent: false, pendingDedents: 0 };
      }
      return context;
    }
    if (term === Dedent) {
      if (context.pendingDedents > 0) {
        return { stack: context.stack, pendingIndent: false, pendingDedents: context.pendingDedents - 1 };
      }
      return context;
    }
    return context;
  },

  reduce(context) {
    return context;
  },

  hash(context) {
    let h = context.stack.length * 65599;
    for (const n of context.stack) h = ((h << 5) + h + n) | 0;
    h = (h + (context.pendingIndent ? 317 : 0) + context.pendingDedents * 1049) | 0;
    return h;
  },
});

export const externalTokenizer = new ExternalTokenizer((input, stack) => {
  const context = stack.context;

  // 1. Emit pending Indent (zero-width)
  if (context.pendingIndent) {
    input.acceptToken(Indent);
    return;
  }

  // 2. Emit pending Dedent (zero-width)
  if (context.pendingDedents > 0) {
    input.acceptToken(Dedent);
    return;
  }

  // 3. At EOF — only emit Newline if we're still inside an indented block
  //    (stack depth > 1). This triggers dedents via the context tracker.
  //    Do NOT emit at the top level, or the parser loops on zero-width Newlines.
  if (input.next === -1 && context.stack.length > 1) {
    input.acceptToken(Newline);
    return;
  }

  // 4. At a newline character — consume \\n and any following blank lines,
  //    but do NOT consume the indentation whitespace of the next content line.
  //    The context tracker's shift(Newline) will peek at that whitespace to
  //    determine indent/dedent, and @skip will consume it afterwards.
  if (input.next === newlineChar) {
    input.advance();
    // Skip blank lines: lines that are empty or contain only whitespace
    for (;;) {
      if (input.next === newlineChar) {
        input.advance();
        continue;
      }
      if (input.next === space || input.next === tab) {
        // Peek to check if this whitespace line is blank
        let offset = 0;
        while (input.peek(offset) === space || input.peek(offset) === tab) offset++;
        const after = input.peek(offset);
        if (after === newlineChar || after === -1) {
          // Blank line with trailing whitespace — consume it all
          for (let i = 0; i <= offset; i++) input.advance();
          continue;
        }
      }
      break;
    }
    input.acceptToken(Newline);
    return;
  }
}, { contextual: true });
`;
}

/**
 * Generate a stub external tokenizer for unknown token patterns
 */
function generateGenericStub(
  termImports: string,
  names: Set<string>,
): string {
  const tokenChecks = [...names]
    .map(name => {
      const pascal = toPascalCase(name);
      return `  // TODO: implement ${name} token logic
  // input.acceptToken(${pascal});`;
    })
    .join('\n\n');

  return `// External tokenizer stub
// Generated by treelsp — you must implement the token logic
import { ExternalTokenizer } from "@lezer/lr";
import { ${termImports} } from "./parser.terms.js";

export const externalTokenizer = new ExternalTokenizer((input, stack) => {
${tokenChecks}
});
`;
}
